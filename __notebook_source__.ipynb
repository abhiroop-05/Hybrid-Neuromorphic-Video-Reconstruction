get_ipython().getoutput("pip install thop")


import os, time, math, psutil, gc, random
import numpy as np
import cv2
import pandas as pd
from collections import namedtuple, deque
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.gridspec import GridSpec
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.onnx 
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
from thop import profile

# ============ PLOTTING CONFIG ============
plt.style.use('seaborn-v0_8-paper')
plt.rcParams.update({
    'font.size': 12,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'figure.titlesize': 16,
    'figure.dpi': 300
})

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running on: {DEVICE}")

# ============ DIRECTORIES ============
BASE_OUT_DIR = "./paper_experiments_v2_final"
PLOTS_DIR = os.path.join(BASE_OUT_DIR, "plots")
MODELS_DIR = os.path.join(BASE_OUT_DIR, "models")
VIDEO_OUT_DIR = os.path.join(BASE_OUT_DIR, "video_outputs")
os.makedirs(PLOTS_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(VIDEO_OUT_DIR, exist_ok=True)

# ============ CONFIGURATION ============
AblationConfig = namedtuple("AblationConfig", [
    "name", 
    "use_hybrid",
    "use_spike",
    "use_analog",
    "use_dog",
    "use_grad_loss",
    "use_contrast_loss",
    "enc_k",
    "decoder_size"
])

# ============ RESOURCE MONITOR ============
class ExperimentMonitor:
    def __init__(self):
        self.gpu_peak = 0
        self.cpu_peak = 0
        self.gpu_memory_history = []
        self.cpu_memory_history = []
        self.inference_times = []
        self.loss_history = []
        
    def start_run(self):
        torch.cuda.reset_peak_memory_stats()
        gc.collect()
        torch.cuda.empty_cache()
        self.inference_times = []
        self.loss_history = []
        self.gpu_memory_history = []
        self.cpu_memory_history = []
        
    def snapshot(self):
        if torch.cuda.is_available():
            current_gpu = torch.cuda.max_memory_allocated() / (1024**2)
            self.gpu_peak = max(self.gpu_peak, current_gpu)
            self.gpu_memory_history.append(current_gpu)
        process = psutil.Process(os.getpid())
        current_cpu = process.memory_info().rss / (1024**2)
        self.cpu_peak = max(self.cpu_peak, current_cpu)
        self.cpu_memory_history.append(current_cpu)
        
    def log_inference(self, duration):
        self.inference_times.append(duration)
        
    def log_loss(self, loss):
        self.loss_history.append(loss)
        
    def get_stats(self):
        avg_inf = np.mean(self.inference_times) if self.inference_times else 0
        fps = 1.0 / avg_inf if avg_inf > 0 else 0
        return {
            "gpu_mem_mb": self.gpu_peak,
            "cpu_mem_mb": self.cpu_peak,
            "inf_time_ms": avg_inf * 1000,
            "fps": fps
        }

def calculate_flops(model, input_shape):
    """Calculate FLOPs for model"""
    dummy_input = torch.randn(*input_shape).to(DEVICE)
    try:
        flops, params = profile(model, inputs=(dummy_input,), verbose=False)
        return flops / 1e9
    except Exception as e:
        print(f"FLOPs calculation failed: {e}")
        return 0.0

# ============ UTILITY FUNCTIONS ============
def make_gaussian_kernel(ksize, sigma, device):
    ax = torch.arange(ksize, dtype=torch.float32, device=device) - (ksize - 1) / 2.0
    xx = ax.view(1, -1).repeat(ksize, 1)
    yy = xx.t()
    kern = torch.exp(-(xx**2 + yy**2) / (2.0 * sigma**2))
    return kern / kern.sum()

def dog_torch(x, ksize=9, sigma1=1.0, sigma2=2.0):
    """Difference of Gaussians"""
    device = x.device
    k1 = make_gaussian_kernel(ksize, sigma1, device).view(1, 1, ksize, ksize)
    k2 = make_gaussian_kernel(ksize, sigma2, device).view(1, 1, ksize, ksize)
    pad = ksize // 2
    return F.conv2d(x, k1, padding=pad) - F.conv2d(x, k2, padding=pad)

# ============ NEURAL NETWORK LAYERS ============

class HybridLIFLayer(nn.Module):
    """Original Hybrid LIF (Spike + Analog)"""
    def __init__(self, in_ch, out_ch, ksize=3, thresh=0.015):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, ksize, padding=ksize//2)
        self.thresh = thresh
        
    def forward(self, x):
        mem = self.conv(x)
        spikes = torch.sigmoid((mem - self.thresh) * 10.0)
        analog = torch.tanh(mem)
        return spikes, analog


class HybridLIFLayer_v2(nn.Module):
    """IMPROVED: Learned weighted fusion instead of concatenation"""
    def __init__(self, in_ch, out_ch, ksize=3, thresh=0.015):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, ksize, padding=ksize//2)
        self.thresh = thresh
        self.fusion_alpha = nn.Parameter(torch.tensor(0.5))
        self.layer_norm = nn.InstanceNorm2d(out_ch)
        
    def forward(self, x):
        mem = self.conv(x)
        mem = self.layer_norm(mem)
        
        spikes = torch.sigmoid((mem - self.thresh) * 10.0)
        analog = torch.tanh(mem)
        
        fused = self.fusion_alpha * spikes + (1 - self.fusion_alpha) * analog
        return torch.cat([fused, spikes, analog], dim=1), spikes, mem


class HighFidelityEncoder(nn.Module):
    def __init__(self, config: AblationConfig, use_v2=False):
        super().__init__()
        self.config = config
        self.k_half = config.enc_k // 2
        self.use_v2 = use_v2
        
        self.out_channels = 0
        if config.use_hybrid:
            self.out_channels = config.enc_k * (3 if use_v2 else 2)
        else:
            if config.use_spike:
                self.out_channels += self.k_half
            if config.use_analog:
                self.out_channels += self.k_half
            
        if use_v2:
            self.layer = HybridLIFLayer_v2(1, self.k_half, ksize=5)
        else:
            self.layer = HybridLIFLayer(1, self.k_half, ksize=5)

    def forward(self, retina_diff):
        if self.use_v2:
            result, spikes, mem = self.layer(retina_diff)
            if self.config.use_hybrid:
                return result, spikes, mem
            elif self.config.use_spike:
                return spikes, spikes, mem
            elif self.config.use_analog:
                return mem, spikes, mem
        else:
            spikes, analog = self.layer(retina_diff)
            if self.config.use_hybrid:
                return torch.cat([spikes, analog], dim=1), spikes, analog
            elif self.config.use_spike:
                return spikes, spikes, analog
            elif self.config.use_analog:
                return analog, spikes, analog


class FidelityLoss(nn.Module):
    """Multi-component loss: L1 + Gradient + Contrast"""
    def __init__(self, config: AblationConfig):
        super().__init__()
        self.config = config
        
    def get_grads(self, img):
        kx = torch.tensor([[-1, 0, 1]], device=img.device, dtype=torch.float32).view(1, 1, 1, 3)
        ky = torch.tensor([[-1], [0], [1]], device=img.device, dtype=torch.float32).view(1, 1, 3, 1)
        return F.conv2d(img, kx, padding=(0, 1)), F.conv2d(img, ky, padding=(1, 0))
        
    def forward(self, pred, target):
        loss = torch.abs(pred - target).mean()
        
        if self.config.use_grad_loss:
            pgx, pgy = self.get_grads(pred)
            tgx, tgy = self.get_grads(target)
            grad_loss = torch.abs(pgx - tgx).mean() + torch.abs(pgy - tgy).mean()
            loss += 0.5 * grad_loss
            
        if self.config.use_contrast_loss:
            pred_std = torch.std(pred)
            target_std = torch.std(target)
            contrast_loss = torch.abs(pred_std - target_std)
            loss += 0.1 * contrast_loss
            
        return loss


# ============ DECODER ARCHITECTURES ============

class ResBlock(nn.Module):
    """Residual block for decoder"""
    def __init__(self, ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(ch, ch, 3, padding=1),
            nn.BatchNorm2d(ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(ch, ch, 3, padding=1),
            nn.BatchNorm2d(ch)
        )
        
    def forward(self, x):
        return F.relu(x + self.net(x))


class FidelityDecoder_Large(nn.Module):
    """IMPROVED: Larger decoder with more capacity"""
    def __init__(self, in_ch, T):
        super().__init__()
        self.T = T
        
        self.entry = nn.Conv2d(in_ch, 128, 3, padding=1)
        self.d1 = nn.Sequential(*[ResBlock(128) for _ in range(3)])
        self.pool1 = nn.Conv2d(128, 256, 3, stride=2, padding=1)
        
        self.d2 = nn.Sequential(*[ResBlock(256) for _ in range(3)])
        self.pool2 = nn.Conv2d(256, 512, 3, stride=2, padding=1)
        
        self.lstm_conv = nn.Conv2d(512 + 512, 4 * 512, 3, padding=1)
        
        self.u1 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1)
        self.d_res1 = nn.Sequential(*[ResBlock(256) for _ in range(3)])
        
        self.u2 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)
        self.d_res2 = nn.Sequential(*[ResBlock(128) for _ in range(3)])
        
        self.out_head = nn.Conv2d(128, 1, 1)

    def lstm_step(self, x, h, c):
        gates = self.lstm_conv(torch.cat([x, h], dim=1))
        i, f, o, g = torch.chunk(gates, 4, dim=1)
        i = torch.sigmoid(i)
        f = torch.sigmoid(f)
        o = torch.sigmoid(o)
        g = torch.tanh(g)
        c = f * c + i * g
        h = o * torch.tanh(c)
        return h, c

    def forward(self, x):
        B, Ctot, H, W = x.shape
        C = Ctot // self.T
        x_seq = x.view(B, self.T, C, H, W)
        
        h = torch.zeros(B, 512, H//4, W//4, device=x.device)
        c = torch.zeros_like(h)
        skip_d1 = None
        skip_d2 = None
        
        for t in range(self.T):
            xt = x_seq[:, t]
            e1 = self.d1(self.entry(xt))
            e2 = self.d2(self.pool1(e1))
            e3 = self.pool2(e2)
            
            if t == self.T - 1:
                skip_d1 = e1
                skip_d2 = e2
                
            h, c = self.lstm_step(e3, h, c)
            
        up1 = self.d_res1(self.u1(h) + skip_d2)
        up2 = self.d_res2(self.u2(up1) + skip_d1)
        return torch.sigmoid(self.out_head(up2))


# ============ DATA PROCESSING ============

def process_video_to_features(video_path, encoder, config: AblationConfig, process_size=(320, 180)):
    """Extract features from video"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"Could not open {video_path}")
    
    feats = []
    targets = []
    spike_activities = []
    ret, frame = cap.read()
    if not ret:
        return [], [], []
    
    frame = cv2.resize(frame, process_size)
    prev_gray = (cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0)
    
    sobel_x = torch.tensor([[-1, 0, 1]], device=DEVICE, dtype=torch.float32).view(1, 1, 1, 3)
    sobel_y = torch.tensor([[-1], [0], [1]], device=DEVICE, dtype=torch.float32).view(1, 1, 3, 1)
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        cur_gray = (cv2.cvtColor(cv2.resize(frame, process_size), cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0)
        t_prev = torch.tensor(prev_gray, device=DEVICE).view(1, 1, process_size[1], process_size[0])
        t_cur = torch.tensor(cur_gray, device=DEVICE).view(1, 1, process_size[1], process_size[0])
        
        diff = t_cur - t_prev
        
        if config.use_dog:
            retina = diff + 0.5 * dog_torch(t_cur)
        else:
            retina = diff
        
        with torch.no_grad():
            result = encoder(retina)
            if isinstance(result, tuple):
                encoded, spikes, mem = result
            else:
                encoded = result
                spikes = None
        
        if spikes is not None:
            spike_rate = spikes.mean().item()
            spike_activities.append(spike_rate)
        else:
            spike_activities.append(0.0)
        
        gx = F.conv2d(t_cur, sobel_x, padding=(0, 1))
        gy = F.conv2d(t_cur, sobel_y, padding=(1, 0))
        mag = torch.sqrt(gx**2 + gy**2 + 1e-6)
        
        full = torch.cat([encoded, mag], dim=1)
        feats.append(full.squeeze(0).cpu().numpy().astype(np.float16))
        targets.append(cur_gray.astype(np.float32))
        prev_gray = cur_gray
    
    cap.release()
    return feats, targets, spike_activities


class SeqDataset(Dataset):
    """Sequential temporal dataset"""
    def __init__(self, f, t, temporal_context=4):
        self.f = f
        self.t = t
        self.T = temporal_context
        
    def __len__(self):
        return len(self.f) - self.T
        
    def __getitem__(self, i):
        seq = [self.f[i + j] for j in range(self.T)]
        x = np.concatenate(seq, axis=0)
        y = self.t[i + self.T - 1]
        return torch.from_numpy(x).float(), torch.from_numpy(y).float().unsqueeze(0)


class AugmentedDataset(Dataset):
    """Dataset with data augmentation"""
    def __init__(self, f, t, temporal_context=4):
        self.f = f
        self.t = t
        self.T = temporal_context
        
    def __len__(self):
        return len(self.f) - self.T
        
    def __getitem__(self, i):
        seq = [self.f[i + j] for j in range(self.T)]
        x = np.concatenate(seq, axis=0)
        y = self.t[i + self.T - 1]
        
        if random.random() > 0.5:
            brightness_factor = random.uniform(0.9, 1.1)
            x = np.clip(x * brightness_factor, 0, 1)
            y = np.clip(y * brightness_factor, 0, 1)
        
        if random.random() > 0.5:
            contrast_factor = random.uniform(0.9, 1.1)
            x_mean = x.mean()
            x = np.clip((x - x_mean) * contrast_factor + x_mean, 0, 1)
            
        if random.random() > 0.7:
            noise = np.random.randn(*x.shape) * 0.01
            x = np.clip(x + noise, 0, 1)
        
        return torch.from_numpy(x).float(), torch.from_numpy(y).float().unsqueeze(0)


def ssim_torch(img1, img2, window_size=11):
    """Compute SSIM between two images"""
    real_size = min(window_size, img1.size(2), img1.size(3))
    sigma = 1.5
    gauss = torch.arange(real_size, dtype=torch.float32, device=img1.device) - real_size // 2
    gauss = torch.exp(-gauss**2 / (2 * sigma**2))
    gauss = gauss / gauss.sum()
    
    window = gauss.unsqueeze(1).mm(gauss.unsqueeze(0)).unsqueeze(0).unsqueeze(0)
    window = window.expand(1, 1, real_size, real_size)
    
    mu1 = F.conv2d(img1, window, padding=window_size//2)
    mu2 = F.conv2d(img2, window, padding=window_size//2)
    
    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2
    
    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2) - mu1_sq
    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2) - mu2_sq
    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2) - mu1_mu2
    
    C1 = 0.01**2
    C2 = 0.03**2
    
    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2)) / ((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))
    return ssim_map.mean()


# ============ VIDEO GENERATION ============

def generate_comparison_video_with_metrics(encoder, decoder, video_path, config: AblationConfig, output_path, process_size=(320, 180)):
    """Generate video with original, reconstructed, and spike activity graph"""
    print(f"\n{'='*70}")
    print(f"Generating comparison video from: {os.path.basename(video_path)}")
    print(f"{'='*70}")
    
    encoder.eval()
    decoder.eval()
    
    feats, targets, spike_activities = process_video_to_features(video_path, encoder, config, process_size)
    
    if len(feats) == 0:
        print("ERROR: No frames extracted from video")
        return
    
    print(f"Extracted {len(feats)} frames")
    print(f"Spike activities collected: {len(spike_activities)}")
    
    dataset = SeqDataset(feats, targets)
    
    predictions = []
    psnr_list = []
    ssim_list = []
    
    print("Generating predictions...")
    loader = DataLoader(dataset, batch_size=1, shuffle=False)
    
    with torch.no_grad():
        for x, y in tqdm(loader, desc="Processing frames"):
            x = x.to(DEVICE)
            y = y.to(DEVICE)
            
            pred = decoder(x)
            predictions.append(pred.squeeze().cpu().numpy())
            
            p_np = pred.squeeze().cpu().numpy()
            y_np = y.squeeze().cpu().numpy()
            
            mse = np.mean((p_np - y_np) ** 2)
            psnr = 20 * math.log10(1.0 / math.sqrt(mse)) if mse > 0 else 100
            psnr_list.append(psnr)
            
            ssim_val = ssim_torch(pred, y).item()
            ssim_list.append(ssim_val)
    
    print(f"Generated {len(predictions)} predictions")
    print(f"Mean PSNR: {np.mean(psnr_list):.2f} dB")
    print(f"Mean SSIM: {np.mean(ssim_list):.4f}")
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    frame_width = process_size[0] * 2 + 20
    frame_height = process_size[1] + 250
    fps = 30
    
    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))
    
    if not out.isOpened():
        print(f"ERROR: Could not open video writer at {output_path}")
        return
    
    print(f"Video writer initialized: {frame_width}x{frame_height} @ {fps} FPS")
    
    print("Rendering video frames...")
    
    spike_smooth = np.convolve(spike_activities, np.ones(5)/5, mode='same')
    spike_max = np.max(spike_smooth) if np.max(spike_smooth) > 0 else 1.0
    
    num_frames = min(len(predictions), len(targets), len(spike_activities))
    
    for frame_idx in tqdm(range(num_frames), desc="Writing frames"):
        original_frame = (targets[frame_idx + 4] * 255).astype(np.uint8)
        reconstructed_frame = (predictions[frame_idx] * 255).astype(np.uint8)
        
        if original_frame.shape != process_size[::-1]:
            original_frame = cv2.resize(original_frame, process_size)
        if reconstructed_frame.shape != process_size[::-1]:
            reconstructed_frame = cv2.resize(reconstructed_frame, process_size)
        
        original_bgr = cv2.cvtColor(original_frame, cv2.COLOR_GRAY2BGR)
        reconstructed_bgr = cv2.cvtColor(reconstructed_frame, cv2.COLOR_GRAY2BGR)
        
        canvas = np.ones((frame_height, frame_width, 3), dtype=np.uint8) * 255
        
        canvas[0:process_size[1], 0:process_size[0]] = original_bgr
        canvas[0:process_size[1], process_size[0]+20:process_size[0]*2+20] = reconstructed_bgr
        
        cv2.putText(canvas, "ORIGINAL", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        cv2.putText(canvas, "RECONSTRUCTED", (process_size[0]+30, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        
        psnr_text = f"PSNR: {psnr_list[frame_idx]:.2f} dB"
        ssim_text = f"SSIM: {ssim_list[frame_idx]:.4f}"
        frame_text = f"Frame: {frame_idx+1}/{num_frames}"
        
        cv2.putText(canvas, psnr_text, (10, process_size[1]+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 200), 1)
        cv2.putText(canvas, ssim_text, (10, process_size[1]+65), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 200), 1)
        cv2.putText(canvas, frame_text, (10, process_size[1]+90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 200), 1)
        
        graph_height = 120
        graph_width = frame_width - 20
        graph_y_start = process_size[1] + 110
        graph_x_start = 10
        
        cv2.rectangle(canvas, 
                     (graph_x_start, graph_y_start),
                     (graph_x_start + graph_width, graph_y_start + graph_height),
                     (220, 220, 220), -1)
        cv2.rectangle(canvas, 
                     (graph_x_start, graph_y_start),
                     (graph_x_start + graph_width, graph_y_start + graph_height),
                     (0, 0, 0), 2)
        
        visible_range = min(100, frame_idx + 1)
        if visible_range > 1:
            start_idx = max(0, frame_idx + 1 - visible_range)
            
            for i in range(visible_range - 1):
                curr_idx = start_idx + i
                next_idx = curr_idx + 1
                
                if curr_idx < len(spike_smooth) and next_idx < len(spike_smooth):
                    x1 = graph_x_start + int((i / (visible_range - 1)) * graph_width)
                    x2 = graph_x_start + int(((i + 1) / (visible_range - 1)) * graph_width)
                    
                    y1 = graph_y_start + graph_height - int((spike_smooth[curr_idx] / spike_max) * graph_height)
                    y2 = graph_y_start + graph_height - int((spike_smooth[next_idx] / spike_max) * graph_height)
                    
                    cv2.line(canvas, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        current_spike = spike_smooth[frame_idx] if frame_idx < len(spike_smooth) else 0
        spike_text = f"Spike Activity: {current_spike:.4f}"
        cv2.putText(canvas, spike_text, (graph_x_start, graph_y_start - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
        
        cv2.putText(canvas, "0", (graph_x_start - 10, graph_y_start + graph_height + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 0), 1)
        cv2.putText(canvas, f"{spike_max:.3f}", (graph_x_start - 20, graph_y_start + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 0), 1)
        cv2.putText(canvas, "Time ‚Üí", (graph_x_start + graph_width - 30, graph_y_start + graph_height + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 0), 1)
        
        out.write(canvas)
    
    out.release()
    print(f"‚úÖ Video saved to: {output_path}")
    
    create_video_summary_plot(psnr_list, ssim_list, spike_activities, config, os.path.basename(video_path))


def create_video_summary_plot(psnr_list, ssim_list, spike_activities, config, video_name):
    """Create comprehensive summary plot"""
    
    fig = plt.figure(figsize=(18, 10))
    gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)
    
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.plot(psnr_list, color='#1f77b4', linewidth=2, label='PSNR')
    ax1.fill_between(range(len(psnr_list)), psnr_list, alpha=0.3, color='#1f77b4')
    ax1.set_title(f'PSNR Over Time', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Frame Number')
    ax1.set_ylabel('PSNR (dB)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.set_ylim([np.min(psnr_list) - 2, np.max(psnr_list) + 2])
    
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.plot(ssim_list, color='#ff7f0e', linewidth=2, label='SSIM')
    ax2.fill_between(range(len(ssim_list)), ssim_list, alpha=0.3, color='#ff7f0e')
    ax2.set_title(f'SSIM Over Time', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Frame Number')
    ax2.set_ylabel('SSIM Score')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    ax2.set_ylim([0, 1.0])
    
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.plot(spike_activities, color='#2ca02c', linewidth=2, label='Spike Activity')
    ax3.fill_between(range(len(spike_activities)), spike_activities, alpha=0.3, color='#2ca02c')
    ax3.set_title(f'Spike Activity Over Time', fontsize=12, fontweight='bold')
    ax3.set_xlabel('Frame Number')
    ax3.set_ylabel('Spike Rate')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    
    ax4 = fig.add_subplot(gs[1, 0])
    ax4.hist(psnr_list, bins=30, color='#1f77b4', edgecolor='black', alpha=0.7)
    ax4.axvline(np.mean(psnr_list), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(psnr_list):.2f}')
    ax4.set_title('PSNR Distribution', fontsize=12, fontweight='bold')
    ax4.set_xlabel('PSNR (dB)')
    ax4.set_ylabel('Frequency')
    ax4.grid(True, alpha=0.3, axis='y')
    ax4.legend()
    
    ax5 = fig.add_subplot(gs[1, 1])
    ax5.hist(ssim_list, bins=30, color='#ff7f0e', edgecolor='black', alpha=0.7)
    ax5.axvline(np.mean(ssim_list), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(ssim_list):.4f}')
    ax5.set_title('SSIM Distribution', fontsize=12, fontweight='bold')
    ax5.set_xlabel('SSIM Score')
    ax5.set_ylabel('Frequency')
    ax5.grid(True, alpha=0.3, axis='y')
    ax5.legend()
    
    ax6 = fig.add_subplot(gs[1, 2])
    ax6.axis('off')
    
    stats_text = f"""
    VIDEO RECONSTRUCTION METRICS
    {'='*40}
    
    Video: {video_name}
    Method: {config.name}
    
    PSNR Statistics:
      Mean: {np.mean(psnr_list):.2f} dB
      Std Dev: {np.std(psnr_list):.2f} dB
      Min: {np.min(psnr_list):.2f} dB
      Max: {np.max(psnr_list):.2f} dB
    
    SSIM Statistics:
      Mean: {np.mean(ssim_list):.4f}
      Std Dev: {np.std(ssim_list):.4f}
      Min: {np.min(ssim_list):.4f}
      Max: {np.max(ssim_list):.4f}
    
    Spike Activity Statistics:
      Mean: {np.mean(spike_activities):.4f}
      Std Dev: {np.std(spike_activities):.4f}
      Min: {np.min(spike_activities):.4f}
      Max: {np.max(spike_activities):.4f}
    
    Frame Count: {len(psnr_list)}
    """
    
    ax6.text(0.1, 0.95, stats_text, transform=ax6.transAxes, fontsize=10,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.suptitle(f'Video Reconstruction Analysis - {config.name}', fontsize=14, fontweight='bold')
    
    plot_path = os.path.join(PLOTS_DIR, f"video_summary_{config.name}_{os.path.basename(video_name)}.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"‚úÖ Summary plot saved to: {plot_path}")


# ============ TRAINING ============

def train_model(encoder, decoder, train_loader, val_loader, config: AblationConfig, 
                epochs=25, learning_rate=2e-3, model_name="model"):
    """Train encoder + decoder - 25 EPOCHS"""
    
    params = list(encoder.parameters()) + list(decoder.parameters())
    optimizer = optim.AdamW(params, lr=learning_rate, weight_decay=1e-5)
    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    criterion = FidelityLoss(config).to(DEVICE)
    scaler = torch.amp.GradScaler('cuda')
    
    best_val_psnr = -float('inf')
    patience = 2
    patience_counter = 0
    
    train_losses = []
    val_psnrs = []
    gpu_memory_log = []
    cpu_memory_log = []
    
    print(f"\nüöÄ Starting 25-epoch training for {model_name}...")
    
    for epoch in range(epochs):
        encoder.train()
        decoder.train()
        
        epoch_loss = []
        train_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/25 [TRAIN]", leave=True)
        
        for x, y in train_bar:
            x, y = x.to(DEVICE), y.to(DEVICE)
            
            optimizer.zero_grad()
            
            with torch.amp.autocast('cuda'):
                pred = decoder(x)
                loss = criterion(pred, y)
            
            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            epoch_loss.append(loss.item())
            train_bar.set_postfix({"loss": np.mean(epoch_loss[-20:])})
        
        scheduler.step()
        avg_loss = np.mean(epoch_loss)
        train_losses.append(avg_loss)
        
        if torch.cuda.is_available():
            gpu_memory_log.append(torch.cuda.max_memory_allocated() / (1024**2))
        process = psutil.Process(os.getpid())
        cpu_memory_log.append(process.memory_info().rss / (1024**2))
        
        if (epoch + 1) % 4 == 0 or epoch == epochs - 1:
            encoder.eval()
            decoder.eval()
            
            val_psnrs_epoch = []
            val_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/25 [VAL]", leave=False)
            
            with torch.no_grad():
                for x, y in val_bar:
                    x, y = x.to(DEVICE), y.to(DEVICE)
                    pred = decoder(x)
                    
                    p_np = pred.squeeze().cpu().numpy()
                    y_np = y.squeeze().cpu().numpy()
                    
                    mse = np.mean((p_np - y_np) ** 2)
                    psnr = 20 * math.log10(1.0 / math.sqrt(mse)) if mse > 0 else 0
                    val_psnrs_epoch.append(psnr)
            
            avg_val_psnr = np.mean(val_psnrs_epoch)
            val_psnrs.append(avg_val_psnr)
            
            print(f"  Val PSNR: {avg_val_psnr:.2f} dB")
            
            if avg_val_psnr > best_val_psnr:
                best_val_psnr = avg_val_psnr
                patience_counter = 0
                torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f"best_{model_name}.pth"))
                print(f"  ‚úì Best model saved (PSNR: {best_val_psnr:.2f})")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break
    
    plt.figure(figsize=(16, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='Training Loss', linewidth=2, color='blue')
    plt.title(f"{model_name} - Training Loss (25 Epochs)", fontweight='bold')
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.subplot(1, 3, 2)
    val_epochs = [4*i+4 for i in range(len(val_psnrs))]
    plt.plot(val_epochs, val_psnrs, marker='o', label='Val PSNR', linewidth=2, color='green')
    plt.title(f"{model_name} - Validation PSNR (25 Epochs)", fontweight='bold')
    plt.xlabel("Epoch")
    plt.ylabel("PSNR (dB)")
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.subplot(1, 3, 3)
    plt.plot(gpu_memory_log, label='GPU Memory', linewidth=2, color='red', marker='o')
    plt.plot(cpu_memory_log, label='CPU Memory', linewidth=2, color='purple', marker='s')
    plt.title(f"{model_name} - Memory Usage (25 Epochs)", fontweight='bold')
    plt.xlabel("Epoch")
    plt.ylabel("Memory (MB)")
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, f"training_curves_{model_name}.png"), dpi=300, bbox_inches='tight')
    plt.close()
    
    return best_val_psnr


# ============ MAIN EXPERIMENT ============

def run_complete_experiment(configs, train_videos, test_video_for_video_gen, epochs=25, batch_size=4):
    """Run complete experiment with video generation"""
    
    for cfg in configs:
        print(f"\n{'='*60}")
        print(f"Running Experiment: {cfg.name}")
        print(f"{'='*60}")
        
        encoder = HighFidelityEncoder(cfg, use_v2=(cfg.decoder_size == "large")).to(DEVICE)
        
        dummy = torch.randn(1, 1, 180, 320).to(DEVICE)
        with torch.no_grad():
            enc_out_tuple = encoder(dummy)
            if isinstance(enc_out_tuple, tuple):
                enc_out = enc_out_tuple[0]
            else:
                enc_out = enc_out_tuple
        
        c_per_frame = enc_out.shape[1] + 1
        
        if cfg.decoder_size == "large":
            decoder = FidelityDecoder_Large(in_ch=c_per_frame, T=4).to(DEVICE)
        else:
            decoder = FidelityDecoder_Large(in_ch=c_per_frame, T=4).to(DEVICE)
        
        print(f"Encoder output channels: {enc_out.shape[1]}")
        print(f"Decoder input channels: {c_per_frame}")
        
        encoder_flops = calculate_flops(encoder, (1, 1, 180, 320))
        decoder_flops = calculate_flops(decoder, (1, c_per_frame*4, 180, 320))
        total_flops = encoder_flops + decoder_flops
        print(f"FLOPs - Encoder: {encoder_flops:.2f} GFLOPs, Decoder: {decoder_flops:.2f} GFLOPs, Total: {total_flops:.2f} GFLOPs")
        
        print(f"\nProcessing {len(train_videos)} TRAINING videos...")
        all_feats = []
        all_targets = []
        
        for vid_path in train_videos:
            print(f"  Loading {os.path.basename(vid_path)}...")
            feats, targets, _ = process_video_to_features(vid_path, encoder, cfg)
            if feats:
                all_feats.extend(feats)
                all_targets.extend(targets)
                print(f"    ‚úì Extracted {len(feats)} frames")
        
        if not all_feats:
            print(f"ERROR: No training data extracted for {cfg.name}")
            continue
        
        print(f"Total training samples: {len(all_feats)}")
        
        split_idx = int(0.8 * len(all_feats))
        train_feats = all_feats[:split_idx]
        train_targets = all_targets[:split_idx]
        val_feats = all_feats[split_idx:]
        val_targets = all_targets[split_idx:]
        
        train_dataset = AugmentedDataset(train_feats, train_targets)
        val_dataset = SeqDataset(val_feats, val_targets)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"\nTraining {cfg.name}...")
        best_psnr = train_model(encoder, decoder, train_loader, val_loader, cfg, 
                               epochs=25, learning_rate=2e-3, model_name=cfg.name)
        
        decoder.load_state_dict(torch.load(os.path.join(MODELS_DIR, f"best_{cfg.name}.pth")))
        
        print(f"\nGenerating comparison video...")
        video_output_path = os.path.join(VIDEO_OUT_DIR, f"{cfg.name}_comparison.mp4")
        generate_comparison_video_with_metrics(encoder, decoder, test_video_for_video_gen, cfg, 
                                              video_output_path, process_size=(320, 180))
        
        del encoder, decoder
        torch.cuda.empty_cache()
        gc.collect()


# ============ EXECUTION ============

if __name__ == "__main__":
    
    VIDEO_1_PATH = "/kaggle/input/neurodata/neurodata/NTU_fight0003_hit_1.mp4"
    VIDEO_2_PATH = "/kaggle/input/neurodata/neurodata/WhatsApp Video 2025-11-25 at 23.09.12_09e1d1b3.mp4"
    TEST_VIDEO_PATH = "/kaggle/input/neurodata/neurodata/people-detection.mp4"
    
    TRAIN_VIDEOS = [VIDEO_1_PATH, VIDEO_2_PATH]
    TEST_VIDEO_FOR_COMPARISON = TEST_VIDEO_PATH
    
    configs = [
        AblationConfig(
            name="Hybrid-Large-v2",
            use_hybrid=True,
            use_spike=True,
            use_analog=True,
            use_dog=True,
            use_grad_loss=True,
            use_contrast_loss=True,
            enc_k=48,
            decoder_size="large"
        ),
    ]
    
    print("\n" + "="*70)
    print("TRAINING: Hybrid-Large-v2 (25 EPOCHS)")
    print("WITH VIDEO GENERATION & METRICS")
    print("="*70)
    
    run_complete_experiment(
        configs,
        train_videos=TRAIN_VIDEOS,
        test_video_for_video_gen=TEST_VIDEO_FOR_COMPARISON,
        epochs=25,
        batch_size=4
    )
    
    print(f"\n‚úÖ Complete experiment finished!")
    print(f"üìÅ Video outputs saved to: {VIDEO_OUT_DIR}")
    print(f"üìÅ Plots saved to: {PLOTS_DIR}")
    print(f"üìÅ Models saved to: {MODELS_DIR}")

